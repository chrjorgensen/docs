---
title: Continue Integration
---
import { Aside, CardGrid, Card, LinkCard, Steps, Badge } from '@astrojs/starlight/components';
import { Image } from 'astro:assets';

This Document provides a more in depth tutorial and demo for using the Continue VSCode extension with Db2 for i.

## Getting Started: Continue
![alt text](image-17.png)

Continue is the leading open source AI code assistant for VS Code. It provides a wide range of AI features:

* Chat Interface
* Code Completion
* Autocomplete

### Install the Continue extension for VS Code

<Steps>
  1. Install the Continue extension from the [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=Continue.continue).
  ![alt text](image-14.png)
  2. Once Installed, there will be a new icon in your VS Code menu (mine is on the top right). Click on the icon to open the chat window.
  ![alt text](image-15.png)
</Steps>

Once you have the extension installed, you can configure the AI provider you want to use. Continue supports multiple AI providers (including [Watsonx](https://docs.continue.dev/customize/model-providers/more/watsonx)!). You can choose the provider you want to use by clicking on the settings icon in the chat window.

For demonstration purposes, we will use the Ollama Provider for hosting LLMs locally on your machine.

### Setting up Ollama Provider

Here is a step-by-step guide to setting up the Ollama provider with the IBM Granite models in Continue:

#### 1. Install Ollama

Install Ollama on your machine by following the link below:
<LinkCard title="Install Ollama" href="https://ollama.com/download" />

#### 2. Fetch the IBM Granite 3.0 models

The IBM Granite 3.0  models are available in the Ollama model registry. More information about the IBM Granite models can be found [here](https://ollama.com/blog/ibm-granite).

Using the Ollama CLI, fetch the IBM Granite 3.0 8b model by running the following command:
```bash
ollama pull granite3-dense:8b
```

#### 3. Configure the Ollama provider in Continue

Open the VSCode Command Palette (Press ctrl+shift+p) and search for `Continue: open config.json`. This will open the Continue central config file `$HOME/.continue/config.json` in your editor. To enable the Granite models in Ollama, add the following configuration to the `models` section:

```json title="~/.continue/config.json"
"models": [
   {
     "title": "Granite Code 8b",
     "provider": "ollama",
     "model": "granite3-dense:8b"
   }
 ],
```

save this file and select the Granite model in the chat window.

![alt text](image-16.png)

### Examples

Once you have the extension installed and the AI provider configured, you can ask questions about your database using the chat window using the `@db2i` context provider. In Continue, a context provider is very similar to a chat participant in GitHub Copilot. It provides additional context to the AI model to help it generate more accurate SQL queries.

More on context providers can be found [here](https://docs.continue.dev/customize/context-providers/).

**Example 1:** Summarize the columns in the `EMPLOYEE` table

![alt text](image-18.png)

**Example 2:** Get the department name for each employee

![alt text](image-19.png)